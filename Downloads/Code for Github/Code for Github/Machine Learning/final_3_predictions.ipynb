{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import helper_functions as my"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input path for the excel spreadsheet\n",
    "INPUT_PATH = \"../Conversion/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define which institution will be kept seperate\n",
    "testInst = \"y\"\n",
    "insts = [(\"a\", \"MDACC\"), (\"m\", \"MSKCC\"), (\"t\", \"Tokyo\"), (\"y\", \"Yale\")]\n",
    "df_original = pd.DataFrame()\n",
    "for inst in insts:\n",
    "    #excel sheet with all data\n",
    "    df_i = pd.read_excel(INPUT_PATH + inst[1] + \"_final_sigPCs.xlsx\")\n",
    "    df_i['patient number'] = df_i['patient number'].astype(str)\n",
    "    df_i['patient number'] = inst[0] + \"_\" + df_i['patient number']\n",
    "    df_i = df_i.drop([\"days_PVE_follow_up\", \"FLR_increase\"], axis=1)\n",
    "    #seperate the testing institution\n",
    "    if inst[0] == testInst:\n",
    "        df_testInst = df_i\n",
    "        continue\n",
    "    df_i = df_i.dropna(axis=0, how='any')\n",
    "    df_original = pd.concat([df_original, df_i], axis=0)\n",
    "    \n",
    "#format all\n",
    "df_original = df_original.reset_index()\n",
    "df_original = df_original.dropna(axis=0, how='any')\n",
    "df_original = df_original.reset_index()\n",
    "df_original = df_original.drop([\"level_0\", \"index\"], axis=1)\n",
    "\n",
    "#format the separate institution\n",
    "if testInst != \"\":\n",
    "    df_testInst = df_testInst.dropna(axis=0, how='any')\n",
    "    df_testInst = df_testInst.reset_index()\n",
    "    df_testInst = df_testInst.drop([\"index\"], axis=1)\n",
    "    df_testInst.set_index('patient number', drop=False, inplace=True)\n",
    "    df_ess_inst = df_testInst[[\"patient number\", \"follow_up_FLR%\", \"follow_up_TLV\", \"KGR\"]]\n",
    "    df_testInst = my.dropFutureValues(df_testInst)\n",
    "\n",
    "df_original.set_index('patient number', drop=False, inplace=True)\n",
    "\n",
    "#keep Y value before throwing it away\n",
    "df_ess = df_original[[\"patient number\", \"follow_up_FLR%\", \"follow_up_TLV\", \"KGR\"]]\n",
    "\n",
    "#get all X input features\n",
    "df_X = my.dropFutureValues(df_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLR volume classification (> 30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict FLR percentage classification\n",
    "cutOff = 0\n",
    "df_input = df_X\n",
    "df_inputInst = df_testInst[df_input.columns]\n",
    "\n",
    "#initialize dataframes\n",
    "df_important = pd.Series(0.0, index = df_input.columns, name=\"Average feature importance\")\n",
    "df_used = pd.Series(index=df_input.columns, dtype='int', name=\"Features frequency used\")\n",
    "df_avgGuessF = pd.DataFrame(index=df_ess.index)\n",
    "numRuns = 100\n",
    "#initialize lists\n",
    "avg_tpr = []\n",
    "avg_fpr = []\n",
    "avg_inst_tpr = []\n",
    "avg_inst_fpr = []\n",
    "allAcc = []\n",
    "allAuc = []\n",
    "allInstAcc = []\n",
    "allInstAuc = []\n",
    "preds = np.zeros(numRuns)\n",
    "df_avgGuessP = pd.DataFrame(index=df_ess.index)\n",
    "for i in range(numRuns):\n",
    "    #Create a Gaussian Classifier\n",
    "    clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "    # Split dataset into training set and test set\n",
    "    x_train, x_test, y_train, y_test = my.splitXY(df_input, df_ess, col=\"follow_up_FLR%\", classes=[0.30])\n",
    "\n",
    "    #exclude highly correlated features\n",
    "    x_train, to_drop = my.exclude_correlated_features_trainingset(x_train)\n",
    "    x_test = my.drop_correlated_features_evaluation(x_test, to_drop)\n",
    "\n",
    "    #Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    #get prediction\n",
    "    y_pred=clf.predict(x_test)\n",
    "\n",
    "    # Model Accuracy, how often is the classifier correct?\n",
    "    tScore = clf.score(x_test, y_test)\n",
    "\n",
    "    #save important features\n",
    "    feature_imp = pd.Series(clf.feature_importances_,index=x_test.columns)\n",
    "    for feat in feature_imp.index:\n",
    "        df_important[feat] += feature_imp[feat]\n",
    "        \n",
    "    #save guesses\n",
    "    df_thisGuessP = pd.DataFrame(y_pred, index=x_test.index)\n",
    "    df_avgGuessP = df_avgGuessP.join(df_thisGuessP, lsuffix=\"_l\")\n",
    "\n",
    "    # calculate the fpr and tpr for all thresholds of the classification\n",
    "    probs = clf.predict_proba(x_test)\n",
    "    preds = probs[1][:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test[:,1], preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    #save fpr, tpr, and auc\n",
    "    avg_fpr = np.append(avg_fpr, fpr)\n",
    "    avg_tpr = np.append(avg_tpr, tpr)\n",
    "    allAuc = np.append(allAuc, roc_auc)\n",
    "    allAcc = np.append(allAcc, tScore)\n",
    "\n",
    "    #keep track of which ones were dropped\n",
    "    for newName in x_train.columns:\n",
    "        if newName in df_used.index:\n",
    "            df_used[newName] += 1\n",
    "        else:\n",
    "            df_used = df_used.reindex(np.append(df_used.index, newName), fill_value=1)\n",
    "\n",
    "    #SEPARATE INSTITUTION\n",
    "    df_inst = my.drop_correlated_features_evaluation(df_inputInst, to_drop)\n",
    "    #set x and y like other insts\n",
    "    df_instX, df_instY = my.setXY(df_inst, df_ess_inst, col=\"follow_up_FLR%\", classes=[0.30])\n",
    "\n",
    "    #test\n",
    "    probs = clf.predict_proba(df_instX)\n",
    "    pred = probs[1]\n",
    "    test_score = clf.score(df_instX,  df_instY)\n",
    "\n",
    "    #auc curve\n",
    "    fpr, tpr, threshold = metrics.roc_curve(df_instY[:,1], pred[:,1])\n",
    "    inst_auc = metrics.auc(fpr, tpr)\n",
    "    avg_inst_fpr = np.append(avg_inst_fpr, fpr)\n",
    "    avg_inst_tpr = np.append(avg_inst_tpr, tpr)\n",
    "    allInstAuc = np.append(allInstAuc, inst_auc)\n",
    "    allInstAcc = np.append(allInstAcc, test_score)\n",
    "\n",
    "#make it a percentage of total runs\n",
    "df_important = df_important/numRuns\n",
    "\n",
    "#get averages for quick results\n",
    "avgScore = sum(allAcc)/numRuns\n",
    "avgInstScore = sum(allInstAcc)/numRuns\n",
    "avg_roc_auc = sum(allAuc)/numRuns\n",
    "avgInst_auc = sum(allInstAuc)/numRuns\n",
    "print(\"Test acc: \", avgScore, \"\\tInst acc: \", avgInstScore)\n",
    "print(\"Test auc: \", avg_roc_auc, \"\\tInst auc: \", avgInst_auc, \"\\n\")\n",
    "\n",
    "#export auc curve stats to excel spreadsheet\n",
    "fpr_tpr = np.stack((avg_fpr, avg_tpr), axis=1)\n",
    "inst_fpr_tpr = np.stack((avg_inst_fpr, avg_inst_tpr), axis=1)\n",
    "df_auc = pd.DataFrame(fpr_tpr, columns=[\"Test FPR\", \"Test TPR\"])\n",
    "df_instAuc = pd.DataFrame(inst_fpr_tpr, columns=[\"Inst FPR\", \"Inst TPR\"])\n",
    "df_allAuc = pd.concat([df_auc, df_instAuc], axis=1)\n",
    "\n",
    "#export all to excel sheet\n",
    "df_score = pd.DataFrame({'Testing Set Accuracy':allAcc, 'Inst Accuracy':allInstAcc})\n",
    "df_avgAuc = pd.DataFrame({'Testing Set AUC':allAuc, 'Inst AUC':allInstAuc})\n",
    "df_features = pd.concat([df_used, df_important], axis=1)\n",
    "df_bothScores = pd.concat([df_score, df_avgAuc], axis=1)\n",
    "df_export = pd.concat([df_bothScores, df_allAuc, df_features.sort_values(by=\"Average feature importance\", ascending=False)], axis=0)\n",
    "excelName = \"FLRclass_\" + testInst + \"_\" + str(cutOff) + \".xlsx\"\n",
    "df_export.to_excel(excelName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLV prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict follow_up_TLV\n",
    "cutOff = 0.1\n",
    "if cutOff > 0:\n",
    "    #linearly correlated features\n",
    "    df_input = my.getCorrelatedInputs(df_X, df_ess, cutOff, \"follow_up_TLV\", verbose=2, heatmap = False)\n",
    "else:\n",
    "    #all features\n",
    "    df_input = df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict TLV\n",
    "#get columns used in fit\n",
    "df_inputInst = df_testInst[df_input.columns]\n",
    "\n",
    "#initiliaze dataframes and lists\n",
    "df_important = pd.Series(0.0, index = df_input.columns, name=\"Average feature importance\")\n",
    "df_avgGuessV = pd.DataFrame(index=df_ess.index)\n",
    "predInstV = pd.DataFrame(0.0, index=df_ess_inst.index, columns=[\"predicted TLV\"])\n",
    "df_used = pd.Series(index=df_input.columns, dtype='int', name=\"Features frequency used\")\n",
    "numRuns = 100\n",
    "#save max value to get error in original units\n",
    "vol_maxY = abs(np.amax(df_ess[\"follow_up_TLV\"]))\n",
    "allScores = []\n",
    "allInstScores = []\n",
    "for i in range(numRuns):\n",
    "    #Create a Random Forest Regressor\n",
    "    clf=RandomForestRegressor(n_estimators=400, max_depth=4)\n",
    "\n",
    "    # Split dataset into training set and test set\n",
    "    x_train_vol, x_test_vol, y_train_vol, y_test_vol = my.splitXY(df_input, df_ess, col = \"follow_up_TLV\", maxY = vol_maxY)\n",
    "\n",
    "    #exclude highly correlated features\n",
    "    x_train_vol, to_drop = my.exclude_correlated_features_trainingset(x_train_vol)\n",
    "    x_test_vol = my.drop_correlated_features_evaluation(x_test_vol, to_drop)\n",
    "\n",
    "    #Train the model using the training \n",
    "    clf.fit(x_train_vol, y_train_vol)\n",
    "    y_pred=clf.predict(x_test_vol)\n",
    "\n",
    "    # RMSE\n",
    "    tScore = np.sqrt(metrics.mean_squared_error(y_test_vol, y_pred))*vol_maxY\n",
    "    allScores = np.append(allScores, tScore)\n",
    "\n",
    "    #important features\n",
    "    feature_imp = pd.Series(clf.feature_importances_,index=x_train_vol.columns)\n",
    "    for feat in feature_imp.index:\n",
    "        df_important[feat] += feature_imp[feat]\n",
    "\n",
    "    df_thisGuessV = pd.DataFrame(y_pred, index=x_test_vol.index)\n",
    "    df_avgGuessV = df_avgGuessV.join(df_thisGuessV, lsuffix=\"_l\")\n",
    "\n",
    "    #SEPARATE INSTITUTION\n",
    "    #get used features from other institution\n",
    "    df_inst = my.drop_correlated_features_evaluation(df_inputInst, to_drop)\n",
    "    #set x and y like other insts\n",
    "    df_instX, df_instY = my.setXY(df_inst, df_ess_inst, col = \"follow_up_TLV\", maxY = vol_maxY)\n",
    "\n",
    "    #test\n",
    "    y_pred=clf.predict(df_instX)\n",
    "    # RMSE\n",
    "    instRMSE = np.sqrt(metrics.mean_squared_error(df_instY, y_pred))*vol_maxY\n",
    "    allInstScores = np.append(allInstScores, instRMSE)\n",
    "    predInstV[\"predicted TLV\"] += y_pred\n",
    "\n",
    "    #keep track of which ones were dropped\n",
    "    for newName in x_train_vol.columns:\n",
    "        if newName in df_used.index:\n",
    "            df_used[newName] += 1\n",
    "        else:\n",
    "            df_used = df_used.reindex(np.append(df_used.index, newName), fill_value=1)\n",
    "\n",
    "#change from total to average\n",
    "df_important = df_important/numRuns\n",
    "predInstV = predInstV/numRuns * vol_maxY\n",
    "\n",
    "\n",
    "\n",
    "#average results\n",
    "avgScore = sum(allScores)/numRuns\n",
    "avgInstRMSE = sum(allInstScores)/numRuns\n",
    "print(\"Test score: \", avgScore, \"\\tInst score: \", avgInstRMSE, \"\\n\")\n",
    "\n",
    "#export TLV predictions to spreadsheet\n",
    "predV = df_avgGuessV.mean(axis=1)*vol_maxY\n",
    "realV = df_ess[\"follow_up_TLV\"]\n",
    "df_predV = pd.DataFrame(predV, index=df_ess.index, columns=[\"predicted TLV\"])\n",
    "df_predInstV = pd.DataFrame(predV, index=df_ess_inst.index, columns=[\"predicted TLV\"])\n",
    "df_bothPredV = pd.concat([df_predV, predInstV], axis=0)\n",
    "df_bothEss = pd.concat([df_ess[\"follow_up_TLV\"], df_ess_inst[\"follow_up_TLV\"]], axis=0)\n",
    "df_allV = pd.concat([df_bothPredV, df_bothEss], axis=1)\n",
    "df_RMSE = pd.Series([avgScore, avgInstRMSE], index = [\"Testing Set\", \"Separate Inst\"], name=\"RMSE\")\n",
    "df_allRMSE = pd.DataFrame({\"Test RMSE\":allScores, \"Inst RMSE\":allInstScores})\n",
    "df_features = pd.concat([df_used, df_important], axis=1)\n",
    "df_export = pd.concat([pd.concat([df_RMSE], axis=1), df_allRMSE, df_allV, df_features.sort_values(by=\"Average feature importance\", ascending=False)], axis=0)\n",
    "excelName = \"TLVpred_\" + testInst + \"_\" + str(cutOff) + \".xlsx\"\n",
    "df_export.to_excel(excelName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinetic Growth Rate classification (> 2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutOff = 0.13\n",
    "if cutOff > 0:\n",
    "    #linearly correlated\n",
    "    df_input = my.getCorrelatedInputs(df_X, df_ess, cutOff, \"KGR\", verbose=2, heatmap = False)\n",
    "else:\n",
    "    #all inputs\n",
    "    df_input = df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict KGR\n",
    "#predict above or below this value\n",
    "classBounds = [0.00]\n",
    "\n",
    "#initalize dataframes\n",
    "df_inputInst = df_testInst[df_input.columns]\n",
    "df_avgGuessK = pd.DataFrame(index=df_ess.index)\n",
    "df_used = pd.Series(index=df_input.columns, dtype='int', name=\"Features frequency used\")\n",
    "\n",
    "#DEFINE NUMBER OF RUNS HERE\n",
    "numRuns = 100\n",
    "\n",
    "#initalize lists\n",
    "avg_tpr = []\n",
    "avg_fpr = []\n",
    "avg_inst_fpr = []\n",
    "avg_inst_tpr = []\n",
    "\n",
    "allAcc = []\n",
    "allAuc = []\n",
    "allInstAcc = []\n",
    "allInstAuc = []\n",
    "for i in range(numRuns):\n",
    "    #Create a Gaussian Classifier\n",
    "    clf=RandomForestClassifier(n_estimators=100, max_depth=4)\n",
    "\n",
    "    # Split dataset into training set and test set\n",
    "    x_train, x_test, y_train, y_test = my.splitXY(df_input, df_ess, col=\"KGR\", classes=classBounds)\n",
    "\n",
    "    #exclude highly correlated features\n",
    "    x_train, to_drop = my.exclude_correlated_features_trainingset(x_train)\n",
    "    x_test = my.drop_correlated_features_evaluation(x_test, to_drop)\n",
    "\n",
    "    #Train the model using the training \n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred=clf.predict(x_test)\n",
    "\n",
    "    # Model Accuracy, how often is the classifier correct?\n",
    "    tScore = clf.score(x_test, y_test)\n",
    "    probs = clf.predict_proba(x_test)\n",
    "    preds = probs[1][:,1]\n",
    "\n",
    "    # calculate the fpr and tpr for all thresholds of the classification\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test[:,1], preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    avg_fpr = np.append(avg_fpr, fpr)\n",
    "    avg_tpr = np.append(avg_tpr, tpr)\n",
    "    allAuc = np.append(allAuc, roc_auc)\n",
    "    allAcc = np.append(allAcc, tScore)\n",
    "\n",
    "    #keep track of which ones were dropped\n",
    "    for newName in x_train.columns:\n",
    "        if newName in df_used.index:\n",
    "            df_used[newName] += 1\n",
    "        else:\n",
    "            df_used = df_used.reindex(np.append(df_used.index, newName), fill_value=1)\n",
    "\n",
    "    #record the cummulative predictions and accuracies\n",
    "    df_thisGuessK = pd.DataFrame(y_pred.argmax(axis=1), index=x_test.index)\n",
    "    df_avgGuessK = df_avgGuessK.join(df_thisGuessK, lsuffix=\"_l\")\n",
    "\n",
    "    #SEPARATE INSTITUTION\n",
    "    df_inst = my.drop_correlated_features_evaluation(df_inputInst, to_drop)\n",
    "    #set x and y like other insts\n",
    "    df_instX, df_instY = my.setXY(df_inst, df_ess_inst, col=\"KGR\", classes=classBounds)\n",
    "\n",
    "    #test\n",
    "    tInstScore = clf.score(df_instX, df_instY)\n",
    "    probs = clf.predict_proba(df_instX)\n",
    "    preds = probs[1][:,1]\n",
    "\n",
    "    #auc curve\n",
    "    fpr, tpr, threshold = metrics.roc_curve(df_instY[:,1], preds)\n",
    "    inst_auc = metrics.auc(fpr, tpr)\n",
    "    avg_inst_fpr = np.append(avg_inst_fpr, fpr)\n",
    "    avg_inst_tpr = np.append(avg_inst_tpr, tpr)\n",
    "    allInstAuc = np.append(allInstAuc, inst_auc)\n",
    "    allInstAcc = np.append(allInstAcc, tInstScore)\n",
    "\n",
    "#calculate average results\n",
    "avgAcc = sum(allAcc)/numRuns\n",
    "avgInstAcc = sum(allInstAcc)/numRuns\n",
    "avg_roc_auc = sum(allAuc)/numRuns\n",
    "avgInst_auc = sum(allInstAuc)/numRuns\n",
    "print(\"Test acc: \", avgAcc, \"\\tInst acc: \", avgInstAcc)\n",
    "print(\"Test auc: \", avg_roc_auc, \"\\tInst auc: \", avgInst_auc)\n",
    "\n",
    "#export auc curve stats to excel spreadsheet\n",
    "fpr_tpr = np.stack((avg_fpr, avg_tpr), axis=1)\n",
    "inst_fpr_tpr = np.stack((avg_inst_fpr, avg_inst_tpr), axis=1)\n",
    "df_auc = pd.DataFrame(fpr_tpr, columns=[\"Test FPR\", \"Test TPR\"])\n",
    "df_instAuc = pd.DataFrame(inst_fpr_tpr, columns=[\"Inst FPR\", \"Inst TPR\"])\n",
    "df_allFprTpr = pd.concat([df_auc, df_instAuc], axis=1)\n",
    "\n",
    "#export all saved data to spreadsheet\n",
    "df_score = pd.DataFrame([avgAcc, avgInstAcc], index = [\"Testing Set\", \"Separate Inst\"], columns=[\"Avg Accuracy\"])\n",
    "df_avgAuc = pd.DataFrame([avg_roc_auc, avgInst_auc], index = [\"Testing Set\", \"Separate Inst\"], columns=[\"Avg AUC\"])\n",
    "df_allScores = pd.DataFrame({'Testing Set Accuracy':allAcc, 'Inst Accuracy':allInstAcc})\n",
    "df_allAuc = pd.DataFrame({'Testing Set AUC':allAuc, 'Inst AUC':allInstAuc})\n",
    "df_bothScores = pd.concat([df_score, df_avgAuc], axis=1)\n",
    "df_allAccAuc = pd.concat([df_allScores, df_allAuc], axis=1)\n",
    "\n",
    "df_temp = pd.concat([df_bothScores, df_allAccAuc, df_allFprTpr], axis=0)\n",
    "df_export = pd.concat([df_temp, pd.concat([df_used.sort_values(ascending=False)], axis=1)], axis=0)\n",
    "excelName = \"KGRclass_\" + testInst + \"_\" + str(cutOff) + \".xlsx\"\n",
    "df_export.to_excel(excelName) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
